---
title: "RTI - Data Science Exercise"
author: "Tanner Robart"
date: "2/20/17"
output: html_document
---

## Report Summary

This is an analysis of the Census data from 1996, looking at which demographic variables have the greatest effect on whether an individual earns over $50,000 anually.

### The following is the exercise prompt:

* Write a SQL query that creates a consolidated dataset from the normalized tables in the database. In other words, write a SQL query that "flattens" the database to a single table.
* Export the "flattened" table to a CSV file.\n
* Import the "flattened" table (or CSV file) into your open source analytic environment of choice (R, Python, Java, etc.) and stage it for analysis. \n
* Perform some simple exploratory analysis and generate summary statistics to get a sense of what is in the data. You should commit any useful or informative exloratory code. \n
* Split the data into training, validation, and test data sets. \n
* Develop a model that predicts whether individuals, based on the census variables provided, make over $50,000/year. Use over_50k as the target variable. \n
* Commit enough code to reproduce your full model selection process, including your final model and all models developed along the way. \n
* Generate a chart that you feel conveys 1 or more important relationships in the data. \n
* Describe your methodology and results in 1/2 page of writing. \n
* Include the chart(s) generated in Step 7 as part of your write-up. If neccesary, explain how the chart(s) informs your approach.


##Steps 1 and 2
#### Write a SQL query that creates a consolidated dataset from the normalized tables in the database. In other words, write a SQL query that "flattens" the database to a single table.

The following SQL query provides a flattened table that we can export to .csv easily.
```
SELECT
	recs.id,
	recs.age,
	s.name as Sex,
	race.name as Race,
	w.name as Workclass,
	recs.hours_week as Hours_Worked,
	edu.name as Education_Level,
  recs.education_num,
  c.name as Country,
	o.name as Occupation,
	rel.name as Relationship,
  m.name as Marital_Status,
	recs.capital_gain as Capital_Gain,
	recs.capital_loss as Capital_Loss,
	recs.over_50k as Over_50k
	
FROM 'records' recs

join countries c
    on c.id = recs.country_id
join workclasses w
	on w.id = recs.workclass_id
join marital_statuses m
	on m.id = recs.marital_status_id
join education_levels edu
	on edu.id = recs.education_level_id
join occupations o
	on o.id = recs.occupation_id
join sexes s
	on s.id = recs.sex_id
join races race
	on race.id = recs.race_id
join relationships rel
	on rel.id = recs.relationship_id
```

This query gives us a single table that we can now export to csv and load into R. We do this and take a first look at some summary statistics for the data.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#install.packages("data.table")
#install.packages("ggplot2")
#install.packages("GGally")
#install.packages("randomForest")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("ROCR")
#install.packages("pastecs")
#install.packages("psych")

#Loading Packages
library(data.table)
library(ggplot2)
library(GGally)
library(dplyr)
library(ROCR)
library(pastecs)
library(psych)
library(e1071)
library(shiny)
```
```{r}
setwd("C:/Users/trobart/Desktop/data-scientist-exercise01") #setting working directory, replace with your own filepath
census_data <- fread("flattened_census_data.csv") # read in csv file as data.table object
census_data[census_data == '?'] <- NA #replacing '?' entries with NA
census_data$Over_50k <- factor(census_data$Over_50k) #making sure Over-50k is treated as a factor 
head(census_data)
summary(census_data)
```
This summary provides a quick look at the distributions of the continuous variables. It also shows us that the count of Over_50k is 11687 out of 48842, or **~23.9%** of our census_dataset make over 50k. But this is a little messy to read and doesn't provide many useful summary statistics, so lets take a look using the psych package's describe() method.


### Additonal summary statistics
```{r, warning = FALSE}
describe(census_data)
```
This table is easier to read and includes standard deviation, skew, se, and kurtosis information. Many of the categorical variables have NA for their summary statistics because they are not ordinal or continuous. Lets also examine how much missing data we have. 
```{r}
library(Amelia)
missmap(census_data, main = "Missing values vs observed")
```


It looks like Occupation and Workclass are the only variables with significant missing values, and we'll keep that in mind during feature selection later. Now lets take a closer look at our target variable and its relationship to the other features.


### Examining initial relationships between variables using a matrix plot
```{r pressure, warning = FALSE, echo=FALSE}
ggpairs(census_data, columns = c('age', 'Sex', 'Race' , 'Over_50k'), diag=list(continuous="density",   discrete="bar"), axisLabels="show")
```


ggpairs provides an easy first look at feature relationships in the data, and allows us to compare continuous and categorical features without first having to do any formatting or type-casting. This is a little messy, and since ggpairs is faceting the plots by levels, some features with many levels are hard to read, like Race. Despite treating this merely as a first look, we can still learn a lot. From examining this we can see we have about twice as many males as females in this data, and age has a skew of a long right-tailed distribution, median around 33. We can also see that the age distribution for Over_50k earners is older on average than those who make less. Lets take a closer look at the differences between under and over 50k earners within other variables.

```{r}
#pairs for other features that are difficult to read in this format
#ggpairs(census_data, columns = c('Workclass',"Relationship", 'Occupation','Over_50k'), axisLabels="show")
```


<!-- deprecated attempt to add shiny exploration of variables

<!-- Next we provide a small shiny interface to let you explore the features. -->

<!-- Choose a feature to examine: -->
<!-- ```{r echo = FALSE} -->
<!-- selectInput("features", "", choices = names(census_data)) -->
<!-- ``` -->
<!-- See a plot: -->
<!-- ```{r echo = FALSE, warning= FALSE} -->
<!-- renderPlot({ -->
<!--  d <- get(input$features) -->
<!--  ggplot(census_data, aes_string(d, fill = 'Over_50k')) + geom_bar() -->
<!-- }) -->
<!-- ``` -->



#### Over 50k proportion
```{r}
p1 <- ggplot(census_data, aes(Over_50k, fill = Over_50k)) +geom_bar()
print(p1)
```


### Age vs. Over_50k
```{r}
p2 <- ggplot(census_data, aes(x = Over_50k, y = age, fill = Over_50k)) + geom_boxplot()
print(p2)
```


#### Education
```{r}
p3 <- ggplot(census_data, aes(x = Over_50k, y = education_num, fill = Over_50k)) + geom_boxplot()
print(p3)
```

We can clearly see that education has a strong relationship with earning over 50k. Lets take a look at the individual category breakdowns.


```{r}
census_data$Education_Level <- factor(census_data$Education_Level, levels=c("Preschool","1st-4th", "5th-6th", "7th-8th", "9th", "10th", "11th", "12th", "HS-grad", "Prof-school", "Assoc-acdm", "Assoc-voc", "Some-college", "Bachelors", "Masters", "Doctorate"))  #re-ordering factor levels to reflect a more sensible linear order

p4 <- ggplot(census_data, aes(x = Education_Level, fill = Over_50k )) + 
  geom_bar() + 
  theme( axis.text.x = element_text(angle=45))

print(p4)
```


Education level seems to be a very good indicator of earning over 50k, with proportion of over_50k increasing as the education level increases. Lets also take a look at education level to see if it contains any other useful information.

```{r}
p4 <- ggplot(census_data, aes(x = education_num, fill = Over_50k )) + 
  geom_bar() + 
  theme( axis.text.x = element_text(angle=45))
print(p4)
```

#### Hours Worked
```{r}
p3 <- ggplot(census_data, aes(x = Hours_Worked, fill = Over_50k)) +
  geom_bar() + 
  theme( axis.text.x = element_text(angle=45))

print(p3)
```

#### Workclass
```{r}
census_data$Workclass <- factor(census_data$Workclass, levels=c( "NA","Never-worked", "Without-pay","Federal-gov","Local-gov", "State-gov" , "Self-emp-inc", "Self-emp-not-inc", "Private"))
p3 <- ggplot(census_data, aes(x = Workclass, fill = Over_50k)) +
  geom_bar() + 
  theme( axis.text.x = element_text(angle=45))

print(p3)
```


This plot makes it clear that the working class with the greatest proportion of over 50k earners is in the self-employed-inc group. But we have by far the most entries from the Private catagory, with more over_50k earners than any single other workclass including both over and under 50k earners. There are only two entries for Never-worked, and so when fitting models, R does not like when the training set does not contain this factor level, so we will remove it without any issues to model performance.

```{r}
census_data[Workclass == "Never-worked"] <- NA
```


#### Occupation
```{r}
#census_data$Occupation <- factor(census_data$Occupation, levels=c()  #re-ordering factor levels to reflect a more sensible order

p5 <- ggplot(census_data, aes(x = Occupation, fill = Over_50k )) + 
  geom_bar() + 
  theme( axis.text.x = element_text(angle=45))

print(p5)
```

The occupations with the greatest proportion of over 50k earners are Exec-managerial and Prof-speciality, which makes sense because executive managerial positions generally earn the most within a company, and speciality professionals could charge whatever the want for an uncommon skill or service.


#### Relationship
```{r}
p5 <- ggplot(census_data, aes(x = Relationship, fill = Over_50k )) + 
  geom_bar() + 
  theme( axis.text.x = element_text(angle=45))

print(p5)
```

Examining the Relationship variable, we see clearly that husbands and wives are much more likely to earn over 50k compared to the other categories. This may be a good correlated feature in our models, so we should definitely use it, and maybe even split it into a binary of is wife/husband or not.  


#### Capital Gain
```{r}

p6 <- ggplot(census_data, aes(Capital_Gain, fill = Over_50k)) + 
  geom_histogram() + 
  theme( axis.text.x = element_text(angle=45))

print(p6)
```

This one is very interesting, it appears more than **90%** of rows in our data have a value of 0 for capital gains, which is significant bias for this feature. However all rows above a certain capital_gain value are over_50k earners, which may be a useful predictive variable. So instead, it might be useful to make a new feature of only capital gains > 0. This feature might improve model performance over the regular capital gain.

#### Capital Loss
```{r}

p5 <- ggplot(census_data, aes(Capital_Loss, fill = Over_50k)) + 
  geom_histogram() + 
  theme( axis.text.x = element_text(angle=45))

print(p5)
```

The same is true of our Capital_Loss feature as it is of our Capital_Gain, so we might want to consider also throwing out the Capital_Loss values of 0, and only using those > 0 as a model feature.


## Split the data into training, validation, and test data sets. 

```{r}
set.seed(1111) # setting seed for reproducibility
sub <- sample(nrow(census_data), floor(nrow(census_data) * 0.8)) # setting a 80/20 split for train/test 
training <- census_data[sub, ]
testing <- census_data[-sub, ]
```


## Create a model that predicts whether individuals, based on the census variables provided, make over $50,000/year.

Next we will fit a few models and compare their performance. But first we need to perform some feature selection based on our previous explorations.

### Feature selection 

```{r}
# training <- training[ c("Relationship", "Education_Level") := NULL]
# testing <- testing[ c("Relationship", "Education_Level") := NULL]

```


### Random Forest Model
```{r}
library(randomForest) #load random_forest package

# #fit the randomforest model
# model <- randomForest(Over_50k~ age + Race + Workclass + Education_Level,
# 	data = training,
# 	importance=TRUE,
# 	keep.forest=TRUE
# )
# 
# print(model)
# 
# varImpPlot(model, type=1)
# 
# #make predictions about testing data
# predicted <- predict(model, newdata=testing[ ,-1])
# 
# # how much variation does the model account for in the 
# 
# actual <- testing$Over_50k
# rsq <- 1-sum((actual-predicted)^2)/sum((actual-mean(actual))^2)
# print(rsq)
```


### Logistic Regression Model

```{r}
mylogit <- glm(Over_50k ~ ., data = training, family = "binomial")
summary(mylogit)
```

```{r}
anova(mylogit, test="Chisq")
```



### Testing and Validation

```{r}
predictions <- predict(mylogit, testing, type = "response")
mylogitPredictions <- prediction(predictions, list(testing$Over_50k))


# ROC 
mylogitROC <- performance(mylogitPredictions, 'tpr','fpr')
plot(mylogitROC)
mylogitACC <- performance(mylogitPredictions, 'acc')
plot(mylogitACC)

# AUC
mylogitAUC <- unlist(performance(mylogitPredictions, 'auc')@y.values)
mylogitAUC


# determining cutoff to use from specificity and sensitivity

# cutoff <- unlist(mylogitROC@alpha.values)[which.max(unlist(mylogitROC@y.values)+ ( 1 - unlist(mylogitROC@x.values) )-1)]

cutoff <- .5 #using a default cutoff of .5 is reasonable and favors high accuracy, but the true positive and false positive rate tradeoff can be optimized to favor one over the other depending on the situation.


predicted <- ifelse(predictions >= cutoff, 1, 0)
logitConfusion <- table(testing$Over_50k, predicted)
logitTruePos <- logitConfusion[2,2]/sum(logitConfusion[2,])
logitTrueNeg <- logitConfusion[1,1]/sum(logitConfusion[1,])
logitAccuracy <-sum(logitConfusion[2,2],logitConfusion[1,1])/sum(logitConfusion)

logitTruePos
logitTrueNeg
logitAccuracy

```


## Support Vector Machine Model


```{r}
# svm.model <- svm(Over_50k ~ Sex+ age + Race, data = training, cost = 500, gamma = 1, epsilon = .2, kernel = "linear")
# print(svm.model)
# summary(svm.model)
```

### Testing and Validation


## Conclusion